{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595800358164",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent, Take 2\n",
    "\n",
    "This note discuss generalized Gradient Descent.\n",
    "\n",
    "There are three forms of network to consider:\n",
    "* one input, one output\n",
    "* multiple inputs, one output\n",
    "* multiple inputs, multiple outputs\n",
    "\n",
    "The basic steps of learning are the same:\n",
    "\n",
    "1. start with weight(s) initialized to some value\n",
    "2. start with a input(s) and corresponding truth value\n",
    "3. calculate `pred = input(s) x weights`\n",
    "4. calculate `error = (pred-truth)**2`\n",
    "5. calculate `node_delta = (pred-truth)`\n",
    "6. calculate `weight_delta = node_delta x weights`\n",
    "7. learning/weight adjustment, `weight = weight - weight_delta`\n",
    "\n",
    "\n",
    "Grokking has some confusing use of variable names, particularly its `neural_network()` and `*_ele_mul()` both use `input` as the first parameter; Not only they are different to each other, they are also not the same as the actual `input`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "layer_1: [-0.          0.51828245 -0.         -0.        ]\nlayer_2: [0.39194327]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    ''' this function sets all negative number to 0 '''\n",
    "    return (x > 0) * x \n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "streetlights = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "walk_vs_stop = np.array([[1, 1, 0, 0]]).T \n",
    "# this is the same as define column labels like this:\n",
    "# array([\n",
    "#  [1],\n",
    "#  [1],\n",
    "#  [0],\n",
    "#  [0]])\n",
    "\n",
    "# randomly initialize weight matrix: 0 to 1\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) -1 \n",
    "\n",
    "layer_0 = streetlights[0]\n",
    "\n",
    "# the output of layer_1 is passed to relu, where negative value becomes 0\n",
    "# This is the input for next layer, layer_2\n",
    "layer_1 = relu(np.dot(layer_0, weights_0_1)) # np.dot() is essentially weighted sum of inputs\n",
    "layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "print(f\"layer_1: {layer_1}\")\n",
    "print(f\"layer_2: {layer_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "\n",
    "Grokking book is not consistent here:\n",
    "* p.128 use hidden size of 3\n",
    "while the code example uses hidden_size = 4\n",
    "* p.126 weight update by addition, it works since `layer_2_delta = walk_vs_stop[i:i+1] - layer_2` switched the order as well. It can be really confusing if not looked at carefully.\n",
    "* `[i:i+1]` is important here, as it will return correct array shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Error: 0.6342311598444467\nError: 0.35838407676317513\nError: 0.0830183113303298\nError: 0.006467054957103705\nError: 0.0003292669000750734\nError: 1.5055622665134859e-05\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    ''' this function sets all negative number to 0 '''\n",
    "    return (x > 0) * x \n",
    "\n",
    "def relu2deriv(x):\n",
    "    ''' Return 1 for x > 0; return 0 otherwise '''\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "streetlights = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "walk_vs_stop = np.array([[1, 1, 0, 0]]).T \n",
    "\n",
    "# randomly initialize weight matrix: 0 to 1\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1 \n",
    "\n",
    "\n",
    "for it in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        # go through each input\n",
    "        # do forward propergation, which is weighted sum\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "\n",
    "        # REFER TO Step #3\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "        # REFER TO Step #4\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1])**2) \n",
    "\n",
    "        # REFER TO Step #5\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1] )\n",
    "\n",
    "\n",
    "        # NEW, not covered in previous steps\n",
    "        # this line computes the delta at layer_1 given the delta at layer_2\n",
    "        # by taking the layer_2_delta and multiplying it by its connecting \n",
    "        # weights (weights_1_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        # REFER TO Step #6, but calculated different, need some revisit\n",
    "        weight_delta_1_2 = layer_1.T.dot(layer_2_delta)\n",
    "        weight_delta_0_1 = layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "        # update weights\n",
    "        weights_1_2 -= alpha * weight_delta_1_2\n",
    "        weights_0_1 -= alpha * weight_delta_0_1\n",
    "    \n",
    "    # \n",
    "    if (it % 10 == 9):\n",
    "        print(f\"Error: {layer_2_error}\")\n",
    "    \n"
   ]
  }
 ]
}