{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595800358164",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Layer Gradient Descent \n",
    "\n",
    "The basic flow:\n",
    "\n",
    "* We started with 2-layer network, with one input, one output.\n",
    "* We can generalize to (still 2-layer network) (1) multiple inputs, one output and (2) multiple inputs, multiple outputs.\n",
    "\n",
    "The basic learning steps are the same, see below.\n",
    "\n",
    "## Basic steps of learning\n",
    "\n",
    "\n",
    "1. start with weight(s) initialized to some value\n",
    "2. start with a input(s) and corresponding truth value\n",
    "3. calculate `pred = input(s) x weights`\n",
    "4. calculate `error = (pred-truth)**2`\n",
    "5. calculate `node_delta = (pred-truth)`\n",
    "6. calculate `weight_delta = node_delta x weights`\n",
    "7. learning/weight adjustment, `weight = weight - weight_delta`\n",
    "\n",
    "\n",
    "Grokking has some confusing use of variable names, particularly its `neural_network()` and `*_ele_mul()` both use `input` as the first parameter; Not only they are different to each other, they are also not the same as the actual `input`.\n",
    "\n",
    "## Full, Stochastic and Batch Gradient Descent\n",
    "\n",
    "* If you do weights update (or learn) with each example as input, then you have so called **Stochastic gradient descent**;\n",
    "\n",
    "* If you do weights update after all inputs are processed, then you have **Full Gradient Descent**\n",
    "\n",
    "* Somewhere in between, you have **Batch Gradient Descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "\n",
    "Grokking book is not consistent here:\n",
    "* p.128 use hidden size of 3\n",
    "while the code example uses hidden_size = 4\n",
    "* p.126 weight update by addition, it works since `layer_2_delta = walk_vs_stop[i:i+1] - layer_2` switched the order as well. It can be really confusing if not looked at carefully.\n",
    "* `[i:i+1]` is important here, as it will return correct array shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Error: 0.6342311598444467\nError: 0.35838407676317513\nError: 0.0830183113303298\nError: 0.006467054957103705\nError: 0.0003292669000750734\nError: 1.5055622665134859e-05\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    ''' this function sets all negative number to 0 '''\n",
    "    return (x > 0) * x \n",
    "\n",
    "def relu2deriv(x):\n",
    "    ''' Return 1 for x > 0; return 0 otherwise '''\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "streetlights = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "walk_vs_stop = np.array([[1, 1, 0, 0]]).T \n",
    "\n",
    "# randomly initialize weight matrix: 0 to 1\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1 \n",
    "\n",
    "\n",
    "for it in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        # go through each input\n",
    "        # do forward propergation, which is weighted sum\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "\n",
    "        # REFER TO Step #3\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "        # REFER TO Step #4\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1])**2) \n",
    "\n",
    "        # REFER TO Step #5\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1] )\n",
    "\n",
    "\n",
    "        # NEW, not covered in previous steps\n",
    "        # this line computes the delta at layer_1 given the delta at layer_2\n",
    "        # by taking the layer_2_delta and multiplying it by its connecting \n",
    "        # weights (weights_1_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        # REFER TO Step #6, but calculated different, need some revisit\n",
    "        weight_delta_1_2 = layer_1.T.dot(layer_2_delta)\n",
    "        weight_delta_0_1 = layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "        # update weights\n",
    "        weights_1_2 -= alpha * weight_delta_1_2\n",
    "        weights_0_1 -= alpha * weight_delta_0_1\n",
    "    \n",
    "    # \n",
    "    if (it % 10 == 9):\n",
    "        print(f\"Error: {layer_2_error}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a Neural Network\n",
    "\n",
    "The Grokking Ch7 is the best chapter I read so far: the description is consistent, and no code snippet to confuse people. \n",
    "\n",
    "The code, the math, and mental picture come together as far as forward propagation goes. For a 3-layer network:\n",
    "\n",
    "$L_2 = \\text{relu}(L_0 W_0) W_1$\n",
    "\n",
    "It is also important to see the vector-matrix multiplication as multiple (# matrix columns) of weighted sum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}