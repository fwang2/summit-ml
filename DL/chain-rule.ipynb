{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Rule\n",
    "\n",
    "the foundation of chain rule is in basic Calculus:\n",
    "Input $x$ pass through two functions: $G$ and $F$, and output is $f$. We are interested to know the relationship of change of $x$ and change of $f$, which is simply: $\\dfrac{\\partial{f}}{\\partial{x}}$.\n",
    "\n",
    "![chain rule1](../figs/chain-rule1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $x$ may not be the only input to function $G$, what is why we should think about partial diff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local gradient\n",
    "\n",
    "![chain rule2](../figs/chain-rule2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows a function $f$, which takes two input $x$ and $y$, output a $z$:\n",
    "\n",
    "$$z = f(x,y)$$\n",
    "\n",
    "And after $f$, its output can go on to other functions, eventually leading to a loss $L$.\n",
    "\n",
    "Now, the backward propagation process will eventually propagate back a value $\\dfrac{\\partial L}{\\partial z}$. As far as $f$ is concerned, we should take a leap of faith, and take this value as a given.\n",
    "\n",
    "$f$ can be any function that we know how to compute derivative: it can be +, -, *, squared and many many others. Given that we know $z = f(x,y)$, we know how to calculate two values:\n",
    "\n",
    "(1) $\\dfrac{\\partial z}{\\partial x}$\n",
    "(2) $\\dfrac{\\partial z}{\\partial y}$\n",
    "\n",
    "Note that these two values are **local gradient**, it can be calulated when we know the inputs - and we can store the calcuated result ahead of time, during the **forward propagation**.\n",
    "\n",
    "These values are useful when backpropagation comes, where we need to calcualte $\\dfrac{\\partial L}{\\partial x}$ and $\\dfrac{\\partial L}{\\partial y}$.\n",
    "\n",
    "The figure above shows how this partial is done by using the **local gradient** value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}