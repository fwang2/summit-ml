{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is learning?\n",
    "\n",
    "ANN learning is about error attribution: if we can identify and quantify errors, then we can reduce errors. The end result is we (or network) learned. The way we identify errors is mostly **compare** - by comparing prediction output with labels.\n",
    "\n",
    "\n",
    "## How do we measure error?\n",
    "\n",
    "one way is \"mean error square\":\n",
    "\n",
    "```\n",
    "goal_pred = 0.2\n",
    "pred = 0.3\n",
    "error = (pred - goal_pred) ** 2\n",
    "```\n",
    "\n",
    "There are two points to note here:\n",
    "* If we define our goal is to make error = 0. Think of this scenario: the first input give error of +1, second input give error of -1, and your average error = 0. That is *not* what we want. Therefore, the error is always positive, negative error doesn't make sense. it makes sure of it by squaring it. \n",
    "* It amplifies bigger error, and reduce smaller error, which is okay. It could be the properties that we actually want.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We need to learn 3 things:\n",
    "\n",
    "1. Neuron\n",
    "2. How Neuron connects to make forward propgation\n",
    "3. How back propagation works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "\n",
    "<img src=\"../figs/neuron.png\" width=\"640\">\n",
    "\n",
    "what is happening?\n",
    "\n",
    "* Each input is multiplifed by a weight, add up, then add another value called bias.\n",
    "* This \"weighted input\" is passed on to \"activation function\", $f$\n",
    "* Why $f$? we want to constrain the output value to a certain range.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Neuron: feedforward\n",
    "\n",
    "Assume $w_1 = 0.2$, $w_2=0.8$ , $b = 5$, neuron input $x_1 = 3$, $x_2=5$ then: \n",
    "\\begin{align}\n",
    "w \\cdot x + b &= (w_1 * x_1) + (w_2 * x_2) + b \\\\\n",
    "                &= 0.2 * 3 + 0.8 * 5 + 5 \\\\\n",
    "                &= 9.6\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Now, activation function:\n",
    "\n",
    "$$ f(9.6) = 0.9999322758503804 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999322758503804"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        output = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(output)\n",
    "\n",
    "weights = np.array([0.2,0.8])\n",
    "bias = 5\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([3,5])\n",
    "n.feedforward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Build Network with Neurons\n",
    "\n",
    "Here we show a neural network built with 3 neurons.\n",
    "\n",
    "<img src=\"../figs/3neurons.png\" width=\"640\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9900481981330957"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([0.2,0.8])\n",
    "bias = 0\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([3,5])\n",
    "n.feedforward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290879223493065"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([0.2,0.8])\n",
    "bias = 0\n",
    "n3= Neuron(weights, bias)\n",
    "x = np.array([0.99,0.99])\n",
    "n3.feedforward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Network\n",
    "Above code, we manually recycled previous definition of single neuron.\n",
    "we can also code it up as a network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290974422779296"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ANN3:\n",
    "    '''\n",
    "    A neuron network with 3 neurons\n",
    "    2 input\n",
    "    a hidden layer with 2 neurons\n",
    "    a output layer with 1 neurons\n",
    "    each neuron has w as 0.2, 0.8\n",
    "    bias 0\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.2, 0.8])\n",
    "        bias = 0\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        \n",
    "        # the inputs to o1 are outputs from h1 and h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "network = ANN3()\n",
    "x = np.array([3,5])\n",
    "network.feedforward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Learning and Loss Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The meaning of learning\n",
    "\n",
    "Let's say we want to use the above network to do something useful.\n",
    "$x_1$ denotes the input of *weight*, $x_2$ denotes the input of *height*, we have a dataset that tabulate:\n",
    "```\n",
    "Name    Weight     Height     Gender\n",
    "Alice   -2         -1           1\n",
    "Bob      25         6           0\n",
    "Charlie  17         4           0 \n",
    "Diana    -15        -6          1\n",
    "```\n",
    "And we like to use above network to predict that given weight and height, what is the gender?\n",
    "\n",
    "The only parameters we can control is weight: the path way to figure out what weights to use is the process of **learning**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of learning\n",
    "\n",
    "To drive this learning process, we need to define a loss function. \n",
    "\n",
    "The goal to have this loss function is to minimize it until no more room (optimal).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE: Mean Squared Error\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2$$\n",
    "\n",
    "* $n$ is the total number of observations\n",
    "* $y_{\\text{pred}}$: predicted outcome\n",
    "* $y_{\\text{true}}$: ground truth\n",
    "\n",
    "Better prediction means lower loss;\n",
    "\n",
    "Training network means find out a set of weights that can minimize the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length\n",
    "    return ((y_true - y_pred)** 2).mean()\n",
    "\n",
    "y_true = np.array([1,0,0,1])\n",
    "y_pred = np.array([0,0,0,0])\n",
    "mse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "GD is the systematic way of minimizing loss.\n",
    "\n",
    "![](figs/3neurons2.png)\n",
    "\n",
    "In this case, the loss function is a multivariate function:\n",
    "\n",
    "$$L(w_1, w_2, w_3, w_4, w_5, b_1, b_2, b_3)$$\n",
    "\n",
    "\n",
    "The question is: how would $L$ change if we want to change $w_1$? This is a question can be answered by partial derivative: $$\\frac{\\partial L}{\\partial w_1}$$.\n",
    "\n",
    "\n",
    "Assume we have ONE sample in the dataset:\n",
    "\n",
    "```\n",
    "Name    Weight     Height     Gender\n",
    "Alice   -2         -1           1\n",
    "```\n",
    "\n",
    "\n",
    "Then\n",
    "\\begin{align}\n",
    "\\rm{MSE} &= \\frac{1}{1} \\sum_{i=1}^1(y_{\\rm{true}} - y_{\\rm{pred}})^2 \\\\\n",
    "         &= (y_{\\rm{true}} - y_{\\rm{pred}})^2 \\\\\n",
    "         &= (1 - y_{\\rm{pred}})^2\n",
    "\\end{align}\n",
    "\n",
    "We can compute $\\frac{\\partial L}{\\partial y_{\\rm pred}}$ as:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial y_{pred}} = \\frac{\\partial (1-y_{pred})^2}{\\partial y_{pred}} = -2(1-y_{pred}) $$\n",
    "\n",
    "\n",
    "We now figure out: $\\frac{\\partial y_{pred}}{\\partial w_1}$:\n",
    "\n",
    "We use $h_1, h_2, o_1$ be the outputs of the neurons they represent. So in this case:\n",
    "\n",
    "$$y_{pred} = o_1 = f(w_5 h_1 + w_6 h_2 + b_3)$$\n",
    "\n",
    "Since $w_1$ only affects $h_1$ (not $h_2$), then:\n",
    "\n",
    "$$\\frac{\\partial y_{pred}}{\\partial w_1} = \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$$\n",
    "\n",
    "And \n",
    "\n",
    "$$\\frac{\\partial y_{pred}}{\\partial h_1} = w_5 * f'(w_5 h_1 + w_6 h_2 + b_3)$$\n",
    "\n",
    "We also know:\n",
    "\n",
    "$$ h_1 = f(w_1 x_1 + w_2 x_2 + b_1)$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial h_1}{\\partial w_1} = x_1 * f'(w_1 x_1 + w_2 x_2 + b_1)$$\n",
    "\n",
    "And, $x_1$ is weight, $x_2$ is height. \n",
    "\n",
    "$$ f'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = f(x) * (1 - f(x))$$\n",
    "\n",
    "\n",
    "Finally, we come back to the earlier question: how does $L$ changes when $w_1$ changes?\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial w_1}$$\n",
    "\n",
    "We have calculated both terms (to the degree that this can be calcuated). \n",
    "\n",
    "\n",
    "![](figs/manual_derive.png)\n",
    "\n",
    "We can initialize all weights to be 1 and all bias to be 0, then manually calulate:\n",
    "\n",
    "$$h_1 = 0.0474, h_2 = 0.524$$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial y_{pred}} &= -2(1-y_{pred}) = -0.952 \\\\\n",
    "\\frac{\\partial y_{pred}}{\\partial h_1} &= w_5 * f'(w_5 h_1 + w_6 h_2 + b_3) = 0.249$$ \\\\\n",
    "\\frac{\\partial h_1}{\\partial w_1} &= x_1 * f'(w_1 x_1 + w_2 x_2 + b_1) = -0.0904 \\\\\n",
    "\\frac{\\partial L}{\\partial w_1} &= 0.0214 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Training: Stochastic Gradient Descent\n",
    "\n",
    "SGD tells us how to update weights and bias to minimize loss. To update:\n",
    "\n",
    "$$ w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L}{\\partial w_1}$$\n",
    "\n",
    "$\\eta$ is famously known as the learning rate.\n",
    "\n",
    "The update formula says:\n",
    "\n",
    "* If $\\frac{\\partial L}{\\partial w_1}$ is positive, then $w_1$ will decrease, which make $L$ descrease.\n",
    "*  If $\\frac{\\partial L}{\\partial w_1}$ is negative, then $w_1$ will increase, which make $L$ descrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process\n",
    "\n",
    "1. Choose one sample from dataset, that is what \"stochastic\" means.\n",
    "2. Calcualte all partial derivative of loss with respect to weights or bias\n",
    "3. Use update equation to update EACH weight and bias\n",
    "4. Go back to step 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code it up (version 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.967\n",
      "Frank: 0.057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff658e4d320>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load ann3_ver0.py\n",
    "#%%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length\n",
    "    return ((y_true - y_pred)** 2).mean()\n",
    "\n",
    "class ANN3:\n",
    "    '''\n",
    "    2 inputs, a hidden layer, a output layer, see above graph for details\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # self.w1, self.w2, self.w3,self.w4,self.w5,self.w6 = np.random.randn(6)\n",
    "        # self.b1, self.b2, self.b3 = np.random.randn(3)\n",
    "        self.w1, self.w2, self.w3,self.w4,self.w5,self.w6 = np.repeat(0,6)\n",
    "        self.b1, self.b2, self.b3 = np.repeat(0, 3)\n",
    "        self.losses = []\n",
    "        self.epochs = []\n",
    "\n",
    "    def feedforward(self,x):\n",
    "        # x is 2-element array\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "    \n",
    "    def train(self, data, y_trues):\n",
    "        '''\n",
    "        data is (nx2) array, n is number of samples, two element is weight and height\n",
    "        y_trues is the corresponding ground truth for each sample\n",
    "        '''\n",
    "        lr = 0.1 # learning rate\n",
    "        epoches = 1000 # number of times to loop through the entire dataset\n",
    "        for epoch in range(epoches):\n",
    "            for x, y_true in zip(data, y_trues):\n",
    "                \n",
    "                # -----\n",
    "                # do feedforward\n",
    "                # -----\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "                \n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "                \n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                \n",
    "                y_pred = o1\n",
    "                \n",
    "                # -----\n",
    "                # do paritial derivative\n",
    "                # naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                # -----\n",
    "                \n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "                \n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "                \n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "                \n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "                \n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "                \n",
    "                # ------\n",
    "                # update weights and bias\n",
    "                # ------\n",
    "            \n",
    "                # Neuron h1\n",
    "                self.w1 -= lr * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= lr * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= lr * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w3 -= lr * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= lr * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= lr * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "                \n",
    "                # Neuron o1\n",
    "                self.w5 -= lr * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= lr * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= lr * d_L_d_ypred * d_ypred_d_b3\n",
    "                \n",
    "            # --- calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(y_trues, y_preds)\n",
    "                # print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "                self.losses.append(loss)\n",
    "                self.epochs.append(epoch)\n",
    "\n",
    "# define dataset\n",
    "\n",
    "data = np.array([\n",
    "    [-2, -1], # Alice\n",
    "    [25,6], # Bob\n",
    "    [17, 4], # Charlie\n",
    "    [-15, -6], # Diana\n",
    "])\n",
    "y_trues = np.array([\n",
    "    1, # alice\n",
    "    0, # bob\n",
    "    0, # charlie\n",
    "    1, # diana\n",
    "])\n",
    "\n",
    "# Training our NN\n",
    "network = ANN3()\n",
    "network.train(data, y_trues)\n",
    "\n",
    "# Make some predictions\n",
    "emily = np.array([-7, -3])  # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily))\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank))\n",
    "\n",
    "plt.plot(network.epochs, network.losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}